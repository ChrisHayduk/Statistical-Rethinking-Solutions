geom_smooth() +
scale_x_continuous(breaks = seq(1, 10, by = 1)) +
labs(xlab = "Population Rank", ylab = "Diversity Index")
#Chart with top 10 counties by Diversity Index
race_and_ethnicity <- race_and_ethnicity %>% arrange(desc(DiversityIndex))
race_and_ethnicity[1:10,] %>% mutate(Diversity_Rank = 1:10) %>%
ggplot(aes(x=Diversity_Rank, y = total_pop)) +
geom_point(size = 3) +
geom_smooth() +
scale_x_continuous(breaks = seq(1, 10, by = 1)) +
labs(xlab = "Population Rank", ylab = "Diversity Index")
#8 & #9
pal <- colorQuantile(palette = "viridis", domain = race_and_ethnicity$DiversityIndex, n = 10)
race_and_ethnicity %>%
leaflet(width = "100%") %>%
addProviderTiles(provider = "CartoDB.Positron") %>%
addPolygons(label = ~ (paste0(County, " - ", "Diversity Index: ", format(round(DiversityIndex, 2), nsmall = 2))
),
stroke = FALSE,
smoothFactor = 0,
fillOpacity = 0.7,
color = ~ pal(DiversityIndex)) %>%
addLegend("bottomright",
pal = pal,
values = ~ DiversityIndex,
title = "DiversityIndex",
opacity = 1)
#11
household_income <- get_acs(geography = "county", variables = c(medincome = "B19013_001"), state = c("NY", "NJ", "CT"), geometry = TRUE)
household_income <- household_income %>% select(-NAME, -GEOID, -variable)
combined_data <- st_join(race_and_ethnicity, household_income)
combined_data <- combined_data %>% rename(Median_Household_Income = estimate)
ggplot(combined_data, aes(x=DiversityIndex, y = Median_Household_Income)) +
geom_point(mapping = aes(color = State)) +
geom_smooth(mapping = aes(color = State)) +
labs(xlab = "Diversity Index", ylab = "Median Household Income")
library(ggplot2)
library(leaflet)
#6
ggplot(race_and_ethnicity, aes(x=DiversityIndex, fill = State)) + geom_histogram(binwidth = 5) + labs(x = "Diversity Index")
#7
#Chart with top 10 counties by population
race_and_ethnicity <- race_and_ethnicity %>% arrange(desc(total_pop))
race_and_ethnicity[1:10,] %>% mutate(Population_Rank = 1:10) %>%
ggplot(aes(x=Population_Rank, y = DiversityIndex)) +
geom_point(size = 3) +
geom_smooth() +
scale_x_continuous(breaks = seq(1, 10, by = 1)) +
labs(x = "Population Rank", y = "Diversity Index")
#Chart with top 10 counties by Diversity Index
race_and_ethnicity <- race_and_ethnicity %>% arrange(desc(DiversityIndex))
race_and_ethnicity[1:10,] %>% mutate(Diversity_Rank = 1:10) %>%
ggplot(aes(x=Diversity_Rank, y = total_pop)) +
geom_point(size = 3) +
geom_smooth() +
scale_x_continuous(breaks = seq(1, 10, by = 1)) +
labs(x = "Population Rank", y = "Diversity Index")
#8 & #9
pal <- colorQuantile(palette = "viridis", domain = race_and_ethnicity$DiversityIndex, n = 10)
race_and_ethnicity %>%
leaflet(width = "100%") %>%
addProviderTiles(provider = "CartoDB.Positron") %>%
addPolygons(label = ~ (paste0(County, " - ", "Diversity Index: ", format(round(DiversityIndex, 2), nsmall = 2))
),
stroke = FALSE,
smoothFactor = 0,
fillOpacity = 0.7,
color = ~ pal(DiversityIndex)) %>%
addLegend("bottomright",
pal = pal,
values = ~ DiversityIndex,
title = "DiversityIndex",
opacity = 1)
library(ggplot2)
library(leaflet)
#6
ggplot(race_and_ethnicity, aes(x=DiversityIndex, fill = State)) + geom_histogram(binwidth = 5) + labs(x = "Diversity Index", y = "Count", title = "Distribution of the Diversity Index")
choose(n=10, k=0)    # number of ways to choose 0 items from 10 = 1
####
# n choose k (combinations)
choose(n=5, k=3)     # number of ways to choose 3 items from 5 = 10
factorial(x=6)   # 6! = 6*5*4*3*2*1=720
####
# factorial: n! = n*(n-1)*...*2*1
factorial(x=4)   # 4! = 4*3*2*1=24
exp.x <- 0
for(i in 0:5){ # compute E[X] for binom(n=5, p=0.6)
exp.x <- exp.x + i*dbinom(x=i, size=5, prob=0.6)
} # end for loop
exp.x # equals 3
mean(rbinom(n=1000, size=5, prob=0.6))
# computing two cdf values at once
pbinom(q=c(12, 4), size=20, prob=0.75)   # P(X<=12), P(X<=4) when n=20, p=0.75
dbinom(x=0, size=10, prob=0.5) + dbinom(x=1, size=10, prob=0.5) +
dbinom(x=2, size=10, prob=0.5) + dbinom(x=3, size=10, prob=0.5) +
dbinom(x=4, size=10, prob=0.5)
# binomial cdf:
pbinom(q=4, size=10, prob=0.5)           # P(X<=4) when n=10, p=0.5 (approx. 0.377)
####
# binomial quantile function:
qbinom(p=0.5, size=15, prob=0.5)         # 50th percentile (median)
#    when n=15, p=0.5
qbinom(p=c(0.2, 0.6), size=20, prob=0.6) # 20th and 60th percentiles
dev.off() # clear previous plotting parameters
# mean and sd labeled
curve(dnorm(x, mean=0, sd=1), col="darkgreen", xlim=c(-3.5, 3.5), las=TRUE, lwd=2,
xaxt="n", yaxt="n", xlab="X",ylab="", main="Normal Distribution")
abline(v=0, col="black")
text(0.3, 0.05, labels=expression(mu), cex=1.3)
arrows(x0=0, y0=0.25, x1=1, y1=0.25, length=0.1)
text(1.3, 0.25, labels=expression(sigma), cex=1.3)
# normal pdf, assuming X has a normal distribution:
dnorm(x=2, mean=0, sd=1)   # density at x=2 for a N(0,1) distribution
# normal cdf, assuming X has a normal distribution:
pnorm(q=3, mean=0, sd=1)   # P(X <= 3) for a N(0,1) distribution
# normal quantile function, assuming X has a normal distribution:
qnorm(p=0.5, mean=0, sd=1) # 50th percentile (median) for a N(0,1) distribution
# simulating draws from a normal distribution
rnorm(n=50, mean=3, sd=5)  # simulating 50 draws from a N(3,5) distribution
# comparing theory and practice
y <- rnorm(n=1000, mean=0, sd=1)
par(mfrow=c(2,2)) # 2x2 figure with 4 plots max in 2 columns
# empirical pdf
hist(y, freq=FALSE, las=TRUE, main="pdf for N(0,1)", ylim=c(0, 0.5))
# actual pdf for N(0,1)
curve(dnorm(x, mean=0, sd=1), col="blue", from=-4, to=4, add=TRUE, lwd=2, lty=2)
# normal quantile function, assuming X has a normal distribution:
qnorm(p=0.5, mean=0, sd=1) # 50th percentile (median) for a N(0,1) distribution
# normal cdf, assuming X has a normal distribution:
pnorm(q=3, mean=0, sd=1)   # P(X <= 3) for a N(0,1) distribution
# empirical cdf
plot(ecdf(y), las=TRUE, main="cdf for N(0,1)", lwd=2, xlim=c(-4, 4), col="limegreen")
curve(pnorm(q=x, mean=0, sd=1), from=-4, to=4, lty=2, lwd=2, col="blue", add=TRUE)
legend(-4.5, 0.9, legend=c("cdf", "ecdf"), col=c("blue", "limegreen"), lwd=2, lty=c(2, 1), bty="n")
legend(-4.5, 0.9, legend=c("cdf", "ecdf"), col=c("blue", "limegreen"), lwd=2, lty=c(2, 1), bty="n")
###
#  normal quantile function, empirical quantile function for N(0,1)
plot(seq(0, 1, length=100), as.vector(quantile(y, probs=seq(0, 1, length=100))), lwd=2, las=TRUE, type="l",
main="quantile function for N(0,1)", xlab="quantile", ylab="value", col="limegreen")
curve(qnorm(p=x, mean=0, sd=1), from=0, to=1, lty=2, lwd=2, col="blue", add=TRUE)
legend("topleft", legend=c("quantile function", "emperical quantile function"), col=c("blue", "limegreen"), lwd=2, lty=c(2,1), bty="n")
pchisq(344.375, df = 2, lower.tail = FALSE)
1.659277e-75/2
pchisq(13.459, df = 1, lower.tail = FALSE)
0.0002438337/2
install.packages("rethinking")
install.packages("rethinking")
install.packages(c("coda","mvtnorm","devtools","loo"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")
install.packages("knitr")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
1/(2*sqrt(2))+1
1/(2*sqrt(2))+1 + (1/2)*(-1/4)*(2)^(-3/2)
sqrt(2)
sin(pi/4)
cos(pi/4)
choose(1/2, 1)
choose(1/2, 2)
choose(1/2, 3)
install.packages("devtools")
library(devtools)
install.packages(c("colorspace", "curl", "openssl", "tibble"))
install.packages("rmarkdown", dependencies = TRUE)
install.packages("rmarkdown", dependencies = TRUE)
install.packages(c("tidyverse", "gridExtra", "broom"), dependencies = TRUE)
require(tidyverse)
require(gridExtra)
require(broom)
# for plots
theme.info <- theme(plot.title = element_text(size=16, hjust=0.5),
axis.title = element_text(size=14),
axis.text = element_text(size=14))
############################
# 1. Read and process data #
setwd("C:\\Users\\chris\\Desktop\\Applied Regression  Analysis")
## REMEMBER TO SET YOUR WORKING DIRECTORY
y <- read_delim("twins.dat", col_names=TRUE, delim=",", na=".")
head(y[,c("DEDUC1", "DEDUC2", "DLHRWAGE")])
dim(y)
head(y)
require(tidyverse)
require(gridExtra)
require(broom)
# for plots
theme.info <- theme(plot.title = element_text(size=16, hjust=0.5),
axis.title = element_text(size=14),
axis.text = element_text(size=14))
############################
# 1. Read and process data #
setwd("C:\\Users\\chris\\Desktop\\Applied Regression  Analysis")
## REMEMBER TO SET YOUR WORKING DIRECTORY
y <- read_delim("twins.dat", col_names=TRUE, delim=",", na=".")
head(y[,c("DEDUC1", "DEDUC2", "DLHRWAGE")])
dim(y)
head(y)
setwd("C:\\Users\\chris\\Desktop\\Applied Regression  Analysis\\Lecture Code\\Lecture 2")
## REMEMBER TO SET YOUR WORKING DIRECTORY
y <- read_delim("twins.dat", col_names=TRUE, delim=",", na=".")
head(y[,c("DEDUC1", "DEDUC2", "DLHRWAGE")])
dim(y)
head(y)
# histogram for DEDUC1
h.DEDUC1 <- y %>% ggplot(aes(DEDUC1)) + geom_histogram(bins=15, col="black", fill="cadetblue") +
ggtitle("Differences in Self-Reported\nEducation Levels") +
labs(x="Twin 1 - Twin 2 (years)") +
geom_text(label = "roughly\nsymmetric", x=-4, y=75, size=5) +
theme.info
# histogram for DEDUC2
h.DEDUC2 <- y %>% ggplot(aes(DEDUC2)) + geom_histogram(bins=15, col="black", fill="cadetblue") +
ggtitle("Differences in Cross-Reported\nEducation Levels") +
labs(x="Twin 1 - Twin 2 (years)") +
geom_text(label = "slightly\nskewed right", x=-4, y=75, size=5) +
theme.info
# histogram for DLHRWAGE
h.DLHRWAGE <- y %>% ggplot(aes(DLHRWAGE)) + geom_histogram(bins=15, col="black", fill="cadetblue") +
ggtitle("Differences in\nlog(Hourly Wage)") +
labs(x="Twin 1 - Twin 2 (log $)") +
geom_text(label = "slightly\nskewed right", x=-1, y=30, size=5) +
theme.info
ggsave("hist.pdf", plot = grid.arrange(h.DEDUC1, h.DEDUC2, h.DLHRWAGE, ncol=3), width=12, height=4)
nrow(y)
summary(y[,c("DEDUC1", "DEDUC2", "DLHRWAGE")])
round(apply(y[,c("DEDUC1", "DEDUC2", "DLHRWAGE")], 2, sd, na.rm=TRUE), digits=2)
##########################
# 3. Bivariate Summaries #
# 3 versions of the same scatter plot
# multiple points on top of each other
s.1 <- y %>% ggplot(aes(x=DEDUC1, y=DEDUC2)) + geom_point(size=2) +
ggtitle("A. Usual Scatterplot") +
labs(x="Self-Reported (years)", y="Cross-Reported (years)") +
theme.info
# add a bit of noise to each point (good for black and white plots)
s.2 <- y %>% ggplot(aes(x=DEDUC1, y=DEDUC2)) + geom_jitter(size=2) +
ggtitle("B. Jittered Points") +
labs(x="Self-Reported (years)", y="Cross-Reported (years)") +
theme.info
# translucent colors: overlapping points result in a darker blue color
s.3 <- y %>% ggplot(aes(x=DEDUC1, y=DEDUC2)) + geom_point(color="#99000070", size=2) +
ggtitle("C. Translucent Points") +
labs(x="Self-Reported (years)", y="Cross-Reported (years)") +
theme.info
# Note: the color "#99000070" is in hexadecimal format, the last 2 digits specify how translucent the points should be.
ggsave("three_scatter.pdf", plot = grid.arrange(s.1, s.2, s.3, ncol=3), width=12, height=4)
round(cor(y$DEDUC1, y$DEDUC2), digits=4)
lm.y <- lm(DEDUC2 ~ DEDUC1, data=y)
# lm.y is a linear model object
summary(lm.y)
# can also use:
tidy(lm.y) # from package broom()
glance(lm.y) # also from package broom()
lm.y
names(lm.y)       # lists  components within object (see above)
lm.y$coefficients # extracts coefficients
s.5 <- y %>% ggplot(aes(x=DEDUC1, y=DEDUC2)) + geom_point(color="#99000070", size=2) +
ggtitle("Differences in Education Levels\n(Twin 1-Twin 2)") +
labs(x="Self-Reported (years)", y="Cross-Reported (years)") +
geom_abline(intercept= 0 , slope=1, color="gray50", size=2, lty=2) +
geom_abline(intercept= coef(lm.y)[1] , slope=coef(lm.y)[2], color="forestgreen", size=2) +
geom_text(label = "y=x", x=5, y=-2, size=5, color="gray50") +
geom_text(label = "estimated", x=5, y=-3, size=5, color="forestgreen") +
theme.info
ggsave("scatter_line.pdf", s.5, width=6, height=6)
# can also use:
tidy(lm.y) # from package broom()
glance(lm.y) # also from package broom()
#####
# 8A. from summary(), see Multiple R-Squared
summary(lm.y)
#####
# 8B. simple linear regression, square correlation r
cor(y$DEDUC1, y$DEDUC2)^2
# SST
SST <- sum((y$DEDUC2-mean(y$DEDUC2))^2)
# SSE = sum of squared errors (i.e., residuals)
SSE <- sum(lm.y$residuals^2)
# compute R^2:
1 - SSE/SST
require(tidyverse)
require(gridExtra)
require(broom)
# for plots
theme.info <- theme(plot.title = element_text(size=16, hjust=0.5),
axis.title = element_text(size=14),
axis.text = element_text(size=14))
setwd("C:\\Users\\chris\\Desktop\\Applied Regression  Analysis\\Exercises\\Lectures 2 and 3")
## REMEMBER TO SET YOUR WORKING DIRECTORY
y <- read_delim("twins.dat", col_names=TRUE, delim=",", na=".")
head(y[,c("DEDUC1", "DEDUC2", "DLHRWAGE")])
head(y[,c("AGE", "HRWAGEL")])
knitr::opts_chunk$set(echo = TRUE)
y <- y[!is.na(y$AGE) & !is.na(y$HRWAGEL), ]
y
View(y)
y <- y[!is.na(y$AGE) & !is.na(y$HRWAGEL), c(AGE, HRWAGEL)]
y <- y[!is.na(y$AGE) & !is.na(y$HRWAGEL), c("AGE", "HRWAGEL")]
summary(y)
y %>% ggplot(aes(x=DEDUC1, y=DEDUC2)) + geom_point(size=2) +
ggtitle("A. Usual Scatterplot") +
labs(x="Self-Reported (years)", y="Cross-Reported (years)") +
theme.info
y %>% ggplot(aes(x=AGE, y=HRWAGEL)) + geom_point(size=2) +
ggtitle("Scatterplot") +
labs(x="Age (years)", y="Hourly Wage ($)") +
theme.info
cor(y$AGE, y$HRWAGEL)
lm(y$HRWAGEL ~ y$AGE, data = y)
summary(lm.y)
lm.y <- lm(y$HRWAGEL ~ y$AGE, data = y)
summary(lm.y)
summary(manual_residuals)
manual_fitted_y <- intercept + slope*y$AGE
slope <- cor(y$AGE, y$HRWAGEL) * sd(y$HRWAGEL)/sd(x$HRWAGEL)
slope <- cor(y$AGE, y$HRWAGEL) * sd(y$HRWAGEL)/sd(y$HRWAGEL)
intercept <- mean(y$HRWAGEL) - slope * mean(y$AGE)
manual_fitted_y <- intercept + slope*y$AGE
manual_residuals <- y$HRWAGEL - manual_fitted_y
summary(manual_residuals)
summary(lm.y)
print(paste0("Hourly Wage = ", intercept, " + ", slope, "*", "AGE"))
summary(manual_residuals)
print(paste0("Hourly Wage = ", intercept, " + ", slope, "*", "AGE"))
summary(manual_residuals)
print(paste0("Hourly Wage = ", intercept, " + ", slope, "*", "AGE"))
summary(lm.y)
#Manual calculations
slope <- cor(y$AGE, y$HRWAGEL) * sd(y$HRWAGEL)/sd(y$AGE)
intercept <- mean(y$HRWAGEL) - slope * mean(y$AGE)
print(paste0("Hourly Wage = ", intercept, " + ", slope, "*", "AGE"))
#Manually calculate residuals
manual_fitted_y <- intercept + slope*y$AGE
manual_residuals <- y$HRWAGEL - manual_fitted_y
summary(manual_residuals)
lm.y <- lm(y$HRWAGEL ~ y$AGE, data = y)
summary(lm.y)
squared_errors <- manual_residuals^2
mean_errors <- (y$HRWAGEL - mean(y$HRWAGEL))^2
SST <- sum(mean_errors)
SSE <- sum(squared_errors)
r_squared <- 1 - SSE/SST
print(r_squared)
y %>% ggplot(aes(x=AGE, y=HRWAGEL)) + geom_point(size=2) + geom_smooth(method = "lm")
ggtitle("Scatterplot with Regression Line") +
labs(x="Age (years)", y="Hourly Wage ($)") +
theme.info
y %>% ggplot(aes(x=AGE, y=HRWAGEL)) + geom_point(size=2) +
geom_smooth(method = "lm") +
ggtitle("Scatterplot with Regression Line") +
labs(x="Age (years)", y="Hourly Wage ($)") +
theme.info
y$log_HRWAGEL <- log(y$HRWAGEL)
head(y)
y %>% ggplot(aes(x=AGE, y=log_HRWAGEL)) + geom_point(size=2) +
ggtitle("Scatterplot with Regression Line") +
labs(x="Age (years)", y="Hourly Wage ($)") +
theme.info
y$log_HRWAGEL <- log(y$HRWAGEL)
y %>% ggplot(aes(x=AGE, y=log_HRWAGEL)) + geom_point(size=2) +
ggtitle("Scatterplot with Regression Line") +
labs(x="Age (years)", y="Hourly Wage ($)") +
theme.info
lm.log_y <- lm(y$log_HRWAGEL ~ y$AGE, data = y)
summary(lm.log_y)
View(lm.y)
View(lm.y)
squared_errors <- lm.log_y$residuals^2
mean_errors <- (y$log_HRWAGEL - mean(y$log_HRWAGEL))^2
SST <- sum(mean_errors)
SSE <- sum(squared_errors)
r_squared <- 1 - SSE/SST
print(r_squared)
y %>% ggplot(aes(x=AGE, y=log_HRWAGEL)) + geom_point(size=2) +
geom_smooth(method = "lm") +
ggtitle("Scatterplot with Regression Line") +
labs(x="Age (years)", y="Log Hourly Wage (log $)") +
theme.info
y$log_HRWAGEL <- log(y$HRWAGEL)
y %>% ggplot(aes(x=AGE, y=log_HRWAGEL)) + geom_point(size=2) +
ggtitle("Scatterplot with Regression Line") +
labs(x="Age (years)", y="Log Hourly Wage (log $)") +
theme.info
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce
print(d)
library(readxl)
mormon <- read_excel("Mormon.xlsx")
getwd()
library(rethinking)
library(readxl)
getwd()
mormon <- read_excel("Mormon.xlsx")
data(WaffleDivorce)
d <- WaffleDivorce
library(rethinking)
library(readxl)
script.dir <- dirname(sys.frame(1)$ofile)
setwd(script.dir)
mormon <- read_excel("Mormon.xlsx")
data(WaffleDivorce)
d <- WaffleDivorce
library(rethinking)
library(readxl)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
mormon <- read_excel("Mormon.xlsx")
data(WaffleDivorce)
d <- WaffleDivorce
join(d, mormon)
d
Mormon
mormon
print(mormon$State)
print(mormon$`State`)
names(mormon) <- c("Location", "MormonPop", "StatePop", "PercMormon")
mormon
mormon <- mormon[,c("Location", "MormonPop", "PercMormon")]
d
d <- left_join(d, mormon)
d
View(mormon)
View(WaffleDivorce)
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage))/sd(d$MedianAgeMarriage)
d$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage)
d$MormonPop.s <- (d$MormonPop - mean(d$MormonPop))/sd(d$MormonPop)
min(d$Marriage.s)
max(d$Marriage.s)
min(d$MormonPop.s)
max(d$MormonPop.s)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
mormon <- read_excel("Mormon.xlsx")
data(WaffleDivorce)
d <- WaffleDivorce
names(mormon) <- c("Location", "MormonPop", "StatePop", "PercMormon")
mormon <- mormon[,c("Location", "MormonPop", "PercMormon")]
d <- left_join(d, mormon)
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage))/sd(d$MedianAgeMarriage)
d$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage)
d$MormonPop.s <- (d$MormonPop - mean(d$MormonPop))/sd(d$MormonPop)
model <- map(
alist(
Divorce ~ dnorm(mu, sigma),
mu <- a + b1 * Marriage.s + + b2 * MedianAgeMarriage.s + b3 * MormonPop.s,
a ~ dnorm(10, 10),
b1 ~ dnorm(0, 1),
b2 ~ dnorm(0, 1),
b3 ~ dnorm(0, 1),
sigma ~ dunif(0, 10)
), data = d)
Marriage.seq <- seq(from = -2, to = 3, length.out = 30)
MAM.seq <- seq(from = -3, to = 3.5, length.out = 30)
Mormon.seq <- seq(from = -1, to = 6.5, length.out = 30)
precis(model)
plot(precis(model))
d$PercMormon.s <- (d$PercMormon - mean(d$PercMormon))/sd(d$PercMormon)
min(d$PercMormon.s)
max(d$PercMormon.s)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
mormon <- read_excel("Mormon.xlsx")
data(WaffleDivorce)
d <- WaffleDivorce
names(mormon) <- c("Location", "MormonPop", "StatePop", "PercMormon")
mormon <- mormon[,c("Location", "MormonPop", "PercMormon")]
d <- left_join(d, mormon)
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage))/sd(d$MedianAgeMarriage)
d$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage)
d$PercMormon.s <- (d$PercMormon - mean(d$PercMormon))/sd(d$PercMormon)
model <- map(
alist(
Divorce ~ dnorm(mu, sigma),
mu <- a + b1 * Marriage.s + + b2 * MedianAgeMarriage.s + b3 * PercMormon.s,
a ~ dnorm(10, 10),
b1 ~ dnorm(0, 1),
b2 ~ dnorm(0, 1),
b3 ~ dnorm(0, 1),
sigma ~ dunif(0, 10)
), data = d)
precis(model)
plot(precis(model))
?PI
?apply
data("foxes")
d <- foxes
print(sd(d$avgfood))
print(sd(d$area))
model4 <- map(
alist(
weight ~ dnorm(mu, sigma),
mu <- a + b1 * avgfood + b2 * groupsize,
a ~ dnorm(10, 10),
b1 ~ dnorm(0, 100),
b2 ~ dnorm(0, 100),
sigma ~ dunif(0, 10)
), data = d)
precis(model4)
plot(precis(model4))
model5 <- map(
alist(
weight ~ dnorm(mu, sigma),
mu <- a + b1 * avgfood + b2 * groupsize + b3 * area,
a ~ dnorm(10, 10),
b1 ~ dnorm(0, 100),
b2 ~ dnorm(0, 100),
b3 ~ dnorm(0, 100),
sigma ~ dunif(0, 10)
), data = d)
precis(model5)
plot(precis(model5))
post <- extract.samples(model5)
plot(avgfood ~ area, post, col=col.alpha(rangi2, 0.1), pch = 16)
post
setwd("C:\\Users\\chris\\Desktop\\Statistical-Rethinking-Solutions\\Chapter 6 - Overfitting, Regularization, and Information Criteria")
