\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb, mathtools}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{fixltx2e}
\usepackage[shortlabels]{enumitem}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newcommand{\textfrac}[2]{\dfrac{\text{#1}}{\text{#2}}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\title{Statistical Rethinking: Chapter 6 - Overfitting, Regularization, and Information Criteria}

\author{Chris Hayduk}
\date{\today}

\maketitle



\section{Easy}

\begin{problem}{6E1}
\text{ }\\
State the three motivating criteria that define information entropy.
\end{problem}

The three motivating criteria that define information entropy are:
\begin{enumerate}
	\item The measure of uncertainty should be continuous. If it were not, then an arbitrarily small change in any of the probabilities would result in a massive change in uncertainty.
	\item The measure of uncertainty should increase as the number of possible events increases. For example, suppose there are two cities that need weather forecasts. In the first city, it rains on half of the days in the year and is sunny on the others. In the second, it rains, shines, and hails, each on 1 out of every 3 days in the year. We'd like our measure of uncertainty to be larger in the second city, where there is one more kind of event to predict.
	\item The measure of uncertainty should be additive. What this means is that if we first measure the uncertainty about rain or shine (2 possible events) and then the uncertainty about hot or cold (2 different possible events), the uncertainty over the four combinations of these events - rain/hot, rain/cold, shine/hot, shine/cold - should be the sum of the separate uncertainties.
\end{enumerate}

\begin{problem}{6E2}
\text{ }\\
Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70\% of the time. What is the entropy of this coin?
\end{problem}

$$H(p) = -Elog(p_i) = -\sum_{i=1}^{n} p_ilog(p_i) = -(0.7*log(0.7) + 0.3*log(0.3)) \approx 0.610864$$

\begin{problem}{6E3}
\text{ }\\
Suppose a four-sided die is loaded such that, when tossed onto a table, it shows "1" 20\%, "2" 25\%, "3" 25\%, and "4" 30\% of the time. What is the entropy of this die?
\end{problem}

\begin{multline*}
H(p) = -Elog(p_i) = -\sum_{i=1}^{n} p_ilog(p_i) \\ = -(0.2*log(0.2) + 0.25*log(0.25) + 0.25*log(0.25) + 0.3*log(0.3)) \approx 1.37623
\end{multline*}

\begin{problem}{6E4}
\text{ }\\
Suppose another four-sided die is loaded such that it never shows "4". The other three sides show equally often. What is the entropy of this die?
\end{problem}

\begin{multline*}
H(p) = -Elog(p_i) = -\sum_{i=1}^{n} p_ilog(p_i) \\ = -(0.3333*log(0.3333) + 0.3333*log(0.3333) + 0.3333*log(0.3333)) \approx 1.09498
\end{multline*}

\section{Medium}

\begin{problem}{6M1}
\text{ }\\
Write down and compare the definitions of AIC, DIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?
\end{problem}

Information criteria: \\
AIC = D_train + 2p \\
DIC = \overline{D} + (\overline{D} - \hat{D}) = \overline{D} + p_D \\
WAIC = -2(lppd - p_WAIC)

WAIC is the most general, as it does not require a multivariate Guassian posterior as in DIC & AIC, and it does not require uninformative priors as in AIC.

\begin{problem}{6M2}
\text{ }\\
Explain the difference between model \texit{selection} and model \texit{averaging}. What information is lost under model selection? What information is lost under model averaging?
\end{problem}

Model selection involves using DIC/WAIC in combination with the estimates and posterior predictive checks from each model in order to choose the "best" model out of a given set. Model averaging means using DIC/WAIC to construct a posterior predictive distribution that exploits what we know about relative accuracy of the models. This helps guard against overconfidence in model structure, in the same way that using the entire posterior distrbution helps guard against overconfidence in parameter values.

Model selection causes us to lose the information provided by the other models in the set that we're considering. Model averaging causes us to have a much more conservative (ie. wider) interval for $\mu$.

\begin{problem}{6M3}
\text{ }\\
When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values if the models were fit to different numbers of observations? Perform some experiments if you are not sure.
\end{problem}

The model fit to fewer observations will almost always have a better deviance and AIC/DIC/WAIC value because it has been asked to predict less.

\begin{problem}{6M4}
\text{ }\\
What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior becomes more concentrated? Why? Perform some experiments if you are not sure.
\end{problem}

Regularizing priors constrain a model's flexibility. Since the effective number of parameters measures how flexible the model is, as a prior becomes more concentrated, it reduces the effective number of parameters.

\begin{problem}{6M5}
\text{ }\\
Provide an informal explanation of why informative priors reduce overfitting.
\end{problem}

Informative priors are generally very narrow around their mean. That is, they tend to have small standard deviations. Since the machine will be very skeptical of values that are more than two standard deviations above or below the prior's mean, a small standard deviation will restrict the paramter to values close to the prior's mean.

\begin{problem}{6M6}
\text{ }\\
Provide an informal explanation of why overly informative priors result in underfitting.
\end{problem}

An overly informative prior will have an excessively tight prior distribution. As a result, the parameter may be constrained to a range that does not contain its "true" value, thus causing the model to underfit the data.

\section{Hard}

\end{document}
